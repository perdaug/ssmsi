\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{amsmath}

\setlength{\oddsidemargin}{0in}
\setlength{\evensidemargin}{0in}
\setlength{\textwidth}{6.5in}
\setlength{\topmargin}{-.3in}
\setlength{\textheight}{9in}
\setlength{\parskip}{1em}

% \pagestyle{empty}
\graphicspath{ {images/} }
% ___________________________________________________________________

\begin{document}

\begingroup  
  \centering
  \large Report: Experiments on $\alpha$ updates \par
  \large Arijus Pleska \par
\endgroup
% ___________________________________________________________________

\par The purpose of this report is to assesses whether an auto-regressive topic model is capable to recover the $\alpha$ parameter used in the synthetic data generation process. This report is structured in the following order: at the start, the rationale of the used Bayesian methods are covered; then, the experiment settings are defined; in the next stage, we assess the experiment results; finally, the identified issues are outlined to be discussed during the following meeting.
% ___________________________________________________________________

\section*{Preliminaries}

\par In order to understand the experiment settings, it is necessary to be familiar with the auto-regressive and non auto-regressive $\alpha$ priors as well as the Metropolis--Hastings (M--H) algorithm; the expressions of the latter concepts are listed below:
\begin{enumerate}
  \item The auto-regressive $\alpha$ prior:
    \begin{align*}
    p(\alpha_0,\ldots,\alpha_T)=p(\alpha_0)\prod_{t=1}^{T}{p(\alpha_t|\alpha_{t-1})}; \qquad &p(\alpha_t)=f(\alpha_0; 0, \sigma_0^2I), \quad t=0;\\
    & p(\alpha_t)=f(\alpha_t; \alpha_{t-1}, \sigma^2I), \quad t>0.
    \end{align*}
  \item The non auto-regressive $\alpha$ prior:
    \begin{align*}
    p(\alpha_0,\ldots,\alpha_T)=\prod_{t=0}^{T}{p(\alpha_t}); \qquad &p(\alpha_t)=f(\alpha_0; 0, \sigma_0^2I), \quad t \geq 0.
\end{align*}
  \item The rationale of the utilised M--H algorithm variation:\\\\
    For the start, familiarise with the following expressions:
    \begin{align*}
      \dfrac{A(x'|x)}{A(x|x')} = \dfrac{p(x'|x)}{p(x|x')}\cdot \dfrac{q(x'|x)}{q(x|x')} = \dfrac{p(x',x)}{p(x)}\cdot \dfrac{p(x')}{p(x,x')} = \dfrac{p(x')}{p(x)}; \qquad &x' \sim q(x,\delta^2I);\\
      & q = \mathcal{N} \Rightarrow q(x'|x) = q(x|x');
    \end{align*}
    where $x$ is the current state, $x'$ is the proposed state, $A$ is the acceptance distribution, and $q$ is the proposal distribution. Taking the previous results into account, the acceptance rate $r$ is expressed as follows:
    \begin{align*}
      r = \min{\bigg(1, \dfrac{p(x')}{p(x)}\bigg)}.
    \end{align*}
  \item The application of the M--H algorithm to update $\alpha$:\\\\
    Note that the $\alpha$ values are updated independently; that is, the expression for the acceptance rate below is for a single $\alpha$ entry.
    \begin{align*}
      \dfrac{p(z,\alpha^{-tk},\alpha_{t, k}'|X)}{p(z,\alpha|X)} &= \dfrac{p(X|z,\alpha^{-tk},\alpha_{t, k}')\cdot p(z|\alpha^{-tk},\alpha_{t, k}')\cdot p(\alpha^{-tk},\alpha_{t, k}')}{p(X)}\cdot \dfrac{p(X)}{p(X|z,\alpha)\cdot p(z|\alpha)\cdot p(\alpha)}\\
      &= \dfrac{\pi(\alpha_{t}')_k^{z_{t,k}}\cdot \pi(\alpha_{t}')_k\cdot p(\alpha_t'|\alpha_{t-1})\cdot p(\alpha_{t+1}|\alpha_t')}{\pi(\alpha_t)_k^{z_{t,k}}\cdot \pi(\alpha_{t})_k\cdot p(\alpha_t|\alpha_{t-1})\cdot p(\alpha_{t+1}|\alpha_t)}; \qquad t > 0, \quad t \neq T;
    \end{align*}
    where $\pi$ is the softmax function, $\alpha_{t,k}' \sim \mathcal{N}(\alpha_{t, k}, \delta^2)$, and $\alpha^{-tk}$ denotes $\alpha$ without $\alpha_{t,k}'$. It follows that,
    \begin{align*}
      r_{t,k} = \min{\bigg(1, \dfrac{\pi(\alpha_{t}')_k^{z_{t,k}}\cdot \pi(\alpha_{t}')_k\cdot p(\alpha_t'|\alpha_{t-1})\cdot p(\alpha_{t+1}|\alpha_t')}{\pi(\alpha_t)_k^{z_{t,k}}\cdot \pi(\alpha_{t})_k\cdot p(\alpha_t|\alpha_{t-1})\cdot p(\alpha_{t+1}|\alpha_t)}\bigg)}; \qquad t > 0, \quad t \neq T.
    \end{align*}
    For the boundary cases $t=0$ and $t=T$, the $p(\alpha)$ term is proportional to $p(\alpha_{0})\cdot p(\alpha_{1}|\alpha_{0})$ and $p(\alpha_{T}|\alpha_{T-1})$ respectively.

    % \begin{align*}
    %   \dfrac{p(\alpha_{t,k}'|\alpha_{t,k})}{p(\alpha_{t,k}|\alpha_{t,k}')}\cdot \dfrac{q(\alpha_{t,k}'|\alpha_{t,k})}{q(\alpha_{t,k}|\alpha_{t,k}')} = \dfrac{p(\alpha_{t,k}',\alpha_{t,k})}{p(\alpha_{t,k})}\cdot \dfrac{p(\alpha_{t,k}')}{p(\alpha_{t,k},\alpha_{t,k}')}, \qquad &\alpha_{t,k}' \sim \mathcal{N}(\alpha_{t,k},\delta^2),\\ 
    %   &q(\alpha_{t,k}'|\alpha_{t,k}) = q(\alpha_{t,k}|\alpha_{t,k})'.
    % \end{align*}
    %   \Rightarrow \dfrac{p(\alpha_{t,k}')}{p(\alpha_{t,k})}

\end{enumerate}


% ___________________________________________________________________

\section*{The Experiment Settings}

\par The intention of the carried experiments is to identify the optimal settings for the Metropolis--Hastings algorithm application. 
The rationale of the carried experiments is based on generating a corpus with pre-defined $\alpha$ changes. Based on the experiments, we will determine which techniques display higher performance in reproducing the pre-defined $\alpha$ changes. To expand on the corpus generation settings, the parameters used are listed below:
\begin{itemize}
  \item The number of topics: K = 2;
  \item The number of documents (time slices): T = 20;
  \item The size of vocabulary: V = 10;
  \item The number of words per document t: $N_t \sim \mbox{Pois}(\lambda),\quad \lambda = 1000$.
\end{itemize}
Speaking of $\alpha_k$ development over time (documents), $\alpha_0$ is a sine curve and $\alpha_1$ is a cosine curve; the corresponding softmax expressions of the curves (i.e, $\theta = \mbox{softmax}(\alpha)$) are illustrated in Figure 1 below.
\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\textwidth]{alpha_initial}
  \caption{The values of $\mbox{softmax}(\alpha)$ used in the generative process.}
  \label{fig:mu}
\end{figure}
Speaking of $\beta$, it was initially pre-defined and kept constant throughout the dynamic generative process; $\beta$ is illustrated in Figure 2 below.
\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\textwidth]{beta_initial}
  \caption{The values of $\beta$ used in the generative process.}
  \label{fig:beta}
\end{figure}
% ___________________________________________________________________

\section*{The experiment results}

\par The first experiment is focused on discovering an optimal choice of the variances. Note that the first experiment is carried using the auto-regressive model; therefore, three different types of variances were considered: the `basic' variance $\sigma^2_0$, the `auto-regressive' variance $\sigma^2$, and the `proposed' variance $\delta^2$. 

\par For the first experiment, $\delta^2$ was kept constant and set to $1$. Effectively, low $\delta^2$ values suggest that the convergence of $\alpha$ is slow and stable, whereas for high values of $\delta^2$ the convergence is faster and less stable. In both cases, with a high number of iterations, a low-error $\alpha$ value will be found. Therefore, we are focusing to tune only the $\sigma^2_0$ and $\sigma^2$ variances

% \par For both experiments, the number of autoregressive iterations was set to $500$ and $\sigma^2$ was set to $0.01$. The resulting plots of $\mu$ with different values of $\sigma^2_0$ and $\delta^2$ are illustrated in Figures 3, 4, 5 below.

\begin{figure}[H]
  \begin{subfigure}[b]{0.33\textwidth}
    \includegraphics[width=\linewidth]{init_1E-01|basic_1E-02|prop_1E+00|it_500}
  \end{subfigure}%
  \begin{subfigure}[b]{0.33\textwidth}
    \includegraphics[width=\linewidth]{init_1E+00|basic_1E-02|prop_1E+00|it_500}
  \end{subfigure}%
  \begin{subfigure}[b]{0.33\textwidth}
    \includegraphics[width=\linewidth]{init_5E+00|basic_1E-02|prop_1E+00|it_500}
  \end{subfigure}
  \caption{$\mbox{softmax}(\alpha)$ with the lowest value of the initial variance.}\label{fig:animals}
\end{figure}
\begin{figure}[H]
  \begin{subfigure}[b]{0.33\textwidth}
    \includegraphics[width=\linewidth]{init_1E-01|basic_1E-03|prop_1E+00|it_500}
  \end{subfigure}%
  \begin{subfigure}[b]{0.33\textwidth}
    \includegraphics[width=\linewidth]{init_1E+00|basic_1E-03|prop_1E+00|it_500}
  \end{subfigure}%
  \begin{subfigure}[b]{0.33\textwidth}
    \includegraphics[width=\linewidth]{init_5E+00|basic_1E-03|prop_1E+00|it_500}
  \end{subfigure}
  \caption{$\mbox{softmax}(\alpha)$ with the lowest value of the initial variance.}\label{fig:animals}
\end{figure}
\begin{figure}[H]
  \begin{subfigure}[b]{0.33\textwidth}
    \includegraphics[width=\linewidth]{init_1E-01|basic_1E-04|prop_1E+00|it_500}
  \end{subfigure}%
  \begin{subfigure}[b]{0.33\textwidth}
    \includegraphics[width=\linewidth]{init_1E+00|basic_1E-04|prop_1E+00|it_500}
  \end{subfigure}%
  \begin{subfigure}[b]{0.33\textwidth}
    \includegraphics[width=\linewidth]{init_5E+00|basic_1E-04|prop_1E+00|it_500}
  \end{subfigure}
  \caption{$\mbox{softmax}(\alpha)$ with the lowest value of the initial variance.}\label{fig:animals}
\end{figure}


\par The second experiment assesses the impact of the auto-regressive $\alpha$ update. For this reason, the $\alpha$ prior was switched to the non auto-regressive one. The resulting $\theta$ values are illustrated in Figure X below.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\textwidth]{thetas}
  \caption{The topic assignments to the documents.}
  \label{fig:mu}
\end{figure}
% ___________________________________________________________________

% \section*{Observations}

% \par For the last section of this report, I have set some questions to be addressed during next meeting; these are listed below:
% \begin{itemize}
%   \item In some cases, the larger number of iterations reduced the performance in reproducing the initial $\alpha$;
% \end{itemize}
% ___________________________________________________________________


%\bibliography{report_mh_alpha-update}{}
%\bibliographystyle{plain}
\end{document}

%% example.tex
%% Jeremy Singer
%% 16 Oct 12

\documentclass{mpaper}

\usepackage{lipsum}
\usepackage{graphicx}
\usepackage{float}
\graphicspath{{images/}}

\begin{document}

\title{Spatial Smoothing in Mass Spectrometry Imaging}
\author{Arijus Pleska}
\matricnum{2019828P}

% ___________________________________________________________________________
\begin{abstract}
\lipsum[1]
\end{abstract}

% ___________________________________________________________________________
\section{Introduction}

% Purpose:
% - A spatial smoothing application in visual data using unsupervised ML
% - Focus: Metabolomics and topic modelling
\par In this research paper, we assess an application of spatial smoothing in visual data; that is, we induce continuity among data elements. The spatial smoothing application is particularly targeted to be applied for unsupervised pattern recognition. To be more specific, our focus is to model a biomedical application in the field of metabolomics. Furthermore, we limit the scope of the applied unsupervised machine learning techniques to the branch of topic modelling.  

% Basis:
% - Visual data: MSI application in metabolomics
% - MSI data structure
\par The characteristics of our utilised metabolomics data are expressed in the form of mass spectrometry imaging (MSI). Effectively, we use MSI to visualise the metabolomics data in the form of spatial distribution. Speaking of the metabolomics data, it contains information about ionised metabolites. Note that metabolites are molecules produced by the chemical process of metabolism; whereas by ionisation, we refer to the method used to sample metabolites. In other words, MSI data is a visualisation of ion (sampled metabolite) distributions: the complete dataset is the whole image; the image's pixel is a particular sampling region; and each region contains intensities of ions with unique mass-over-charge $m/z$ values. 

% Basis:
% - Unsupervised machine learning: Topic modelling
% - topic modelling w.r.t. MSI
\par Speaking of our machine learning application, topic modelling is a technique used to infer unlabelled topic distributions based on data's underlying semantic structure. With respect to MSI data, we can model the topic distributions over an image and its every pixel; furthermore, topic modelling can express the types of ions corresponding to particular topics. Since a topic model is a statistical approach to perceive real metabolomics data, the utilised topic models are tuned to reflect the metabolomics environment as good as possible. Relating to our research targets, spatial smoothing is one of such environment settings.

% Research Problem:
% - Noisiness
% -- Fragmentation
% -- Retention time
% - Loss of information
% -- Overlapping topics
\par The basis of the project's research problems comes from the limitations of current metabolite sampling techniques. The main limitation is the loss of information caused by the metabolite ionisation. As a consequence, the MSI data is noisy. To expand on the noisiness, it is caused by metabolite fragmentation and the limitation to tune different metabolite retention times. By metabolite fragmentation, we refer to a metabolite split; the split would cause the captured ions to possess unexpected values. Speaking of the retention time, the intensity value of each ion types varies with respect to time; therefore, since each ion is captured at a different state of its retention, each ion type would possess some variance with respect to its intensity. Finally, note that the loss of information might be also caused by overlapping ion topics. Since different ion topics can contain same ion types, the topic possessing a lower ion intensity value would be overwhelmed and, thus, not reflected in the MSI data.
% Contributions:
% - A specific and extensive assessment of the SS application
% - A tuned topic model and well-organised experiment settings
\par In this paper, we contribute to the research in MSI by carrying an extensive assessment of the spatial smoothing application. The assessment is carried in both quantitative and qualitative manners: we assess the performance on a number of diverse datasets; also, our experiments are designed to reflect the nature of the MSI data reflecting computational metabolomics. Furthermore, we provide a Python implementation of a tuned topic model; also, we establish maintainable experiment settings. By the tuned topic model, we mean that the model's implementation is particularly designed to meet the characteristics of the MSI data. Speaking of the experiment settings, note that we are carrying the experiments using Jupyter notebooks. Effectively, the use of the notebooks creates a portable and well-documented environment to initialise the experiment settings and execute the topic modelling. As a result, external parties could run the released notebooks and reproduce the experiment results in a swift manner.

% The paper's organisation
\par The paper is organised in the following order: in Section 2, we discuss the background of the research project; in Section 3, we provide a formal definition of the assessed research problems; in Section 4, we review the results of the relevant research; in Section 5, we establish the rationale of the applied methodology; in Section 6, we introduce the experiments results; finally, in Section 7, we conclude the findings. 

% ___________________________________________________________________________
\section{Background}

% Intro
% - Preliminaries: Introduce the rationale of LDA
% - Terminology: Define the applied topic modelling terminology
% - MSI Data characteristics: Introduce the qualities of the data
\par The background section covers the basis of the concepts used throughout the paper. At the start, we provide a high-level overview of the general topic modelling concepts. Then, we define the terminology used throughout the paper. Finally, we introduce the characteristic qualities of the MSI data.  

\subsection{Topic Modelling Preliminaries}

% LDA Intro:
% - Hierarchical structure
% - Generative model
% - Bayesian methods
% - Assumptions
\par The research project targets a specific branch of topic models. The branch consists of Latent Dirichlet Allocation (LDA) derivatives. Note that the initial LDA model was introduced by Blei et al. \cite{blei2003latent}. One of the model's key characteristics is the three-level hierarchical treatment of the data. In the context of the utilised MSI data, the hierarchical structure can be perceived as follows: in the highest level, we have an MSI image; in the middle level, we have a pixel of an MSI image; and in the lowest level, we have the intensities of particular ions in a pixel.

% Generatitive model:
% - Generative process
\par Another model's key characteristic is the generative treatment of the data. By the generative model, we mean that the latent data instances are treated as a result of a mixture of underlying parameters drawn from probability distributions. In other words, the generative data treatment induces randomness in the end products of the data; however, note that the source of the data -- the lowest level parameters of the probability distributions -- remain the same. The key aspect of the generative model is the degree of freedom in the connections of random variables; this notion allows modelling more realistic, thus, more complex data relations. Ultimately, the rationale of the generative model is based on recovering the underlying semantic structure. Effectively, in order to recover the generative process of the data, we delve into the applications of Bayesian methods.

% Bayesian methods
% - Inference: sampling
% - Connection to LDA
\par By applying Bayes' rule, we can express the underlying semantic structure can be expressed in the form of posterior probability distributions. However, note that we can not analytically compute such posterior expressions. Instead, we can estimate the the posterior distributions using optimisation or direct sampling. In this project, we particularly focus on the underlying semantic structure's inference using direct sampling. Speaking of the sampling-based inference method applications to LDA-like models, the ground-work has been established by Griffiths and Steyvers \cite{griffiths2004finding}. The authors have utilised a collapsed Gibbs sampling -- a Markov chain Monte Carlo (MCMC) algorithm -- in order integrate the uncertainty out and sample only the parameters of interest. More specific details about the rationale of the collapsed Gibbs sampling application will be provided in Section 4. 

% Assumptions
% - Exchengeability (BoW)
% - Discrete data
\par Relating to the initial LDA model, the authors have set the following assumptions for the utilised data: exchangeability among the inner components of the lower and middle data hierarchy levels; and discrete treatment of the lower-level data. By exchangeability, it is meant that the components follow the bag-of-words principle; that is, the order of the data has no correlation with the underlying semantic structure. Speaking of the discrete data treatment, it is assumed that the lower-level components have no spatial connection. With respect to our project, both assumption types -- exchangeability and discreteness -- do not correspond to the characteristics of the MSI data. Therefore, we will establish a methodology to relax the latter assumptions.

\subsection{Topic Modelling Terminology}

% Terminology intro:
% - Introduce the three-level hierarchical structure
\par In this subsection, we introduce the topic modelling terminology. In order to put the MSI data in the topic modelling context, the components of  LDA's three-level hierarchical structure are defined as follows: \textit{the corpus} is the whole image of a sample; \textit{the document} is an MSI image's pixel; and \textit{the word} is an interval of mass-over-charge values corresponding to a particular ion. Starting from now on, we will use the latter topic modelling concepts to introduce the LDA model's architecture and notation.

% LDA design
% - Graphical model
\par Since LDA-like models are also graphical models, the dependence of random variables can be illustrated using graphs. In order to familiarise with the architecture and the variables of LDA-like models, we provide Figure \ref{fig:arch_LDA-init} illustrating the initial LDA model in the plate notation.
\begin{figure}[h]
  \centering
  \includegraphics[width=0.25\textwidth]{LDA-initial.png}
  \caption{The initial LDA model's architecture.}
  \label{fig:arch_LDA-init}
\end{figure}
The circles indicate the model's variables: the coloured circle corresponds to the observable variable, whereas the uncoloured circles corresponds to the hidden variables. Effectively, the hidden variables define the model's underlying structure. Speaking of the plates, the plate denoted by $K$ corresponds to the number of topics; the plate denoted by $T$ corresponds to the number of documents and the plate denoted by $N$ corresponds to the number of words. Effectively, the letters in the bottom right corners indicate the total number of variables. Therefore, a corpus has $T$ number of documents, and each document has $N$ number of words.

% Terminology
% - Parameter meaning
% - Listing
\par Before providing a listing with the definitions of the LDA variables, we will briefly introduce the purpose of the variables. The variables denoted by $\theta$ and $\phi$ correspond to the underlying probability distributions (e.g., in the case of the initial LDA model, we use Dirichlet distributions). It follows that the variables denoted by $\alpha$ and $\beta$ act as the parameters of the latter probability distributions; note that in the context of machine learning, such auxiliary parameters are called hyper-parameters. Effectively, \textit{a hyper-parameter} allows to tune a machine learning model for a particular dataset application. As a side reference, note that by \textit{a vocabulary} we refer to a collection of terms reflecting a fixed range of the words. At this point, we provide the following list containing the definitions of the initial LDA model's variables: 
\begin{itemize}
	\item $K$ is the number of topics;
	\item $T$ is the number of documents;
	\item $N$ is the number of words per document;
	\item $V$ is the size of a vocabulary;
	\item $w$ is a word;
	\item $z$ is a word's topic assignment;
	\item $\theta_t$ is the topic distribution over document $t$;
	\item $\phi_k$ is the vocabulary term distribution over topic $k$;
	\item $\alpha$ is the hyper-parameter for the topic distributions;
	\item $\beta$ is the hyper-parameter for the vocabulary term distributions.
\end{itemize}

\subsection{MSI Data Characteristics}

% MSI dataset intro:
% - Introduce the raw data format
% - Describe the data pre-processing requirements
\par In this subsection, we will introduce the qualities of the raw MSI data. Furthermore, we will set the requirements for the raw data's pre-processing; note that the pre-processing serves as an auxiliary method making the raw MSI data compatible for a scalable topic modelling application. As a side note, the basis of our applied MSI data characteristics is established from the mzML data format. Conveniently, we have parsed the mzML data format using the \texttt{pymzml} Python library. 

% Raw data format
% - Data meaning
% - Explain the raw data by looking into it
\par Essentially, raw MSI data contain mass spectra of a sample -- the sample's every pixel is expressed in the form of a mass spectrum. By a mass spectrum, we refer to a map of from a mass-over-charge value to an intensity value. Ultimately, we can establish a continuous notion of the MSI data by ordering the mass-over-charge values. Note that since the sampling equipment can detect the mass-over-charge values in the millidalton (mDa) precision, the MSI data is sparse (i.e., a large portion of mass-over-charge values are mapped to the zero intensity).

% Data pre-processing requirements
% - Vocabulary generation
% - Intensity normalisation
% - Dismissing inferior information
\par In the context of topic modelling, the raw MSI data possess a large vocabulary (above $5000$ terms). Note that we consider every mass-to-charge value as a word; whereas every intensity value is perceived as a word occurrence count. Further, since the intensity values could spike up above $1000$, the time complexity of the latent topic inference would require pro-longed runs. To overcome the introduced scalability issues, we have carried data pre-processing -- the applied techniques are discussed in the upcoming methodology section.

% ___________________________________________________________________________
\section{Statement of Problem}

% Hypothesis
% - A topic is continuous throughout nearby regions
\par To start with, we set the hypothesis of this research project to \textit{`The noisiness of MSI pixels can be reduced by applying a topic model tuned for spatial smoothing'.} By spatial smoothing, it is meant that the topic model would have an auto-regressive treatment among the pixels. As an example, we assume that adjacent pixels would have similar latent topic distributions. This assumption corresponds to the nature of our datasets -- a metabolite construction (i.e., a topic) is continuous throughout nearby regions (i.e., sets of adjacent pixels).

% Contributions
% - Naive AR model application
% - Motivating for the state-of-the-art AR model application
\par The impact of proving the hypothesis would bring the following contributions:
\begin{itemize}
	\item Improving the detection of overlapping topics;
	\item Reducing the noisiness of MSI data;
	\item Motivating the further research in applying spatial \\smoothing to MSI data.
\end{itemize} 
Speaking of the overlapping topic detection and the reduced noisiness, both contributions would improve the performance of MSI pattern recognition. Additionally, we measure the changes in the performances upon varying the complexity of the data. Ultimately, if a naive spatial smoothing application displayed performance improvements, we would set a basis to apply state-of-the-art auto-regression approaches on MSI data.

% Approach
% - Clearly define the experiment settings
% - Tune the models by studying synthetic datasets.
\par To my knowledge, the impact of the spatial smoothing application to the MSI domain has not yet been thoroughly studied. For the latter reason, this research project will serve as an exploratory assessment on the spatial smoothing application: we will introduce the rationale behind the applied methodology; also, we will clearly define the range of the experiment settings. To give a brief intuition about the methodology, the study will assess the domain-specific parameter tuning and its impact on a diverse range of synthetic datasets.

% ___________________________________________________________________________
\section{Relevant Research}

% Intro (separate into three)
% - MSI DATA:
% -- alonso2015analytical
% -- Review the metabolomics feature pre-processing
% -- Review the metabolomics alternative approaches
% - Review the implementation approaches
% -- blei2012probabilistic
% -- Gibbs sampling
% -- DTM
\par In this section, we will review the groundwork on the following aspects: the pre-processing of MSI data; the rationale of the utilised topic models; and the applications of topic modelling on MSI data. Note that the covered groundwork is selected to reflect the rationale of the utilised techniques. In order to establish the basis of the state-of-the-art computational metabolomics methodology, we have consulted the survey carried by Alonso et al. \cite{alonso2015analytical}; speaking of the key probabilistic topic modelling branches, we have consulted the survey carried by Blei \cite{blei2012probabilistic}. 

\subsection{MSI Data Pre-processing}

% Metabolomics: feature pre-processing
% - Normalisation:
% -- kohl2012state
% -- bolstad2003comparison

\par In order to establish a scalable topic inference, we review the following MSI data pre-processing techniques: data normalisation; feature binning; and noise reduction. To start with, data normalisation serves as a method establishing an integrable data structure. Bolstat et al. \cite{bolstad2003comparison} have proposed a data normalisation method called linear baseline scaling. Note that the method is particularly targeted at sparse datasets. Effectively, the method is applied on numerical features by executing the following work-flow: we find the largest numerical feature of all data instances; then, we calculate the scaling factors by aligning the largest values to a pre-set upper threshold; finally, we align the remaining numerical features based on the established scaling factors. Relating linear baseline scaling to applications on MSI data, Kohl et al. \cite{kohl2012state} have shown that the method's application does not produce a significant loss of information for a well-performing MSI data inference. 

% - Binning
% -- Evenly spaced 
% -- craig2006scaling
% -- Spectral peaks
% -- de2008nmr
\par Speaking of feature binning, the technique is used to merge the instances of a feature space; as a result, feature binning reduces the data dimensionality. Note that by the MSI data feature space, we refer to distinct $m/z$ values. Since raw MSI data is sparse, a successful application of feature binning is based on identifying appropriate $m/z$ boundaries reflecting unique metabolite types. To introduce some examples, the research carried by Craig et al. \cite{craig2006scaling} have utilised an equally spaced feature binning. Even though the authors have succeeded to reduce the data dimensionality, they have encountered a loss of information by splitting the metabolite topics into arbitrary regions. As an alternative approach, De et al. \cite{de2008nmr} have performed a feature binning based on the spectral peak identification. Effectively, the technique induces a dynamic notion of bin boundaries which is based on the identified intensity peak regions. As a result, the merged $m/z$ values serve as a better representation of metabolite terms.

% - Denoising (removing small intensities)
% -- smith2014proteomics
\par The last reviewed MSI data pre-processing techniques addresses the MSI data noisiness. Even though the MSI data noise reduction is an open research problem going well beyond data pre-processing, Smith et al. \cite{smith2014proteomics} have shown that the application of a general data pre-processing routine displays performance improvements. For example, the authors have induced a lower intensity bound. Effectively, the intensity values below the intensity threshold would be treated as insignificant and/or as a product of data capturing device imperfections. By applying this procedure, the authors have successfully reduced the data dimensionality and, thus, increased the data scalability.

\subsection{Prospective Topic Modelling Methodology}

% Intro
% - Inference
% - Derivatives
% - Gibbs sampling
% -- griffiths2004finding
\par In this subsection, we review the key characteristics of the prospective topic modelling inference techniques and model variations. Speaking of the inference, we have the employed the rationale of a sampling-based LDA model. Griffiths and Steyvers \cite{griffiths2004finding} have displayed a successful application of the collapsed Gibbs sampler for the inference of both textual and visual topics. Note that by the collapsed sampler, it is meant that the topic assignment inference is not directly impacted by the measurements of topic and term distributions; instead, the sampling is based on the notion of counts. In mathematical terms, the authors suggest obtaining a topic assignment's probability from the following expression:
\begin{equation}
P(z_i = k | z_{-i}, w) \propto \dfrac{n_{-i, k}^{(w_i)} + \beta}{n_{-i, k}^{(\cdot)} + V\beta}\cdot \dfrac{n_{-i, k}^{(t_i)} + \alpha}{n_{-i, \cdot}^{(t_i)} + K\alpha},
\end{equation}
note that $i$ refers to the current iteration ($-i$ refers to the previous iteration); $k$ refers to a particular topic; and $n$ refers to the count of the instances indicated by the term's superscript. If required, $\theta$ and $\phi$ can be sampled from the following distributions:
\begin{align}
\phi_{k,w} & \sim \mbox{Dirichlet}\bigg(\dfrac{n_{k}^{(w)} + \beta}{n_{k}^{(\cdot)} + W\beta}\bigg),\\
\theta_{t,k} & \sim \mbox{Dirichlet}\bigg(\dfrac{n_{k}^{(t)} + \alpha}{n^{(t)} + K\alpha}\bigg).
\end{align}
Essentially, accurate representations of $\theta$ and $\phi$ are obtained by taking the average of their values generated upon the Gibbs sampler execution's iterations.


% Topic modelling:
% - DTM
% -- blei2006dynamic
% - SeqLDA
% -- du2012sequential
\par \lipsum[1]



% Metabolomics: spatial smoothing
% - Spatial Chaos
% -- palmer2016fdr
% - TM application
% -- van2016topic
\par \lipsum[1]



% ___________________________________________________________________________
\section{Methodology}

\lipsum[1-3]

% ___________________________________________________________________________
 \section{Experiments}

\lipsum[1-3]

% ___________________________________________________________________________
\section{Conclusion}

\lipsum[1]

% ___________________________________________________________________________
\bibliographystyle{abbrv}
\bibliography{bibliography}

\end{document}